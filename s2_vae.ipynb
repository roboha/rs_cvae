{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "\n",
    "from vae import ccvae\n",
    "from load_sentinel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attempt=60\n",
    "\n",
    "bs = 8\n",
    "kp = 0.7\n",
    "train_size = bs * 10 * 5#5 scenes\n",
    "valid_size = bs * 2\n",
    "\n",
    "recording_interval = 100\n",
    "resample_interval = 5000\n",
    "\n",
    "n_filters1 = 64\n",
    "\n",
    "testlogpath = '../vae_logs/test/' + str(attempt)\n",
    "trainlogpath = '../vae_logs/train/' + str(attempt)\n",
    "genlogpath = '../vae_logs/gen/' + str(attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/ramdisk/S2_10_A.tif', '/media/ramdisk/S2_20_A.tif']\n",
      "['/media/ramdisk/S2_10_B.tif', '/media/ramdisk/S2_20_B.tif']\n",
      "['/media/ramdisk/S2_10_C.tif', '/media/ramdisk/S2_20_C.tif']\n",
      "['/media/ramdisk/S2_10_D.tif', '/media/ramdisk/S2_20_D.tif']\n",
      "['/media/ramdisk/S2_10_E.tif', '/media/ramdisk/S2_20_E.tif']\n",
      "['/media/ramdisk/S2_10_A.tif', '/media/ramdisk/S2_20_A.tif']\n",
      "['/media/ramdisk/S2_10_B.tif', '/media/ramdisk/S2_20_B.tif']\n",
      "['/media/ramdisk/S2_10_C.tif', '/media/ramdisk/S2_20_C.tif']\n",
      "['/media/ramdisk/S2_10_D.tif', '/media/ramdisk/S2_20_D.tif']\n",
      "['/media/ramdisk/S2_10_E.tif', '/media/ramdisk/S2_20_E.tif']\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        nov_cnn = ccvae(mb=False, len_edge=32, outer=64,\n",
    "                 middle=128, inner=256, innest=512, h_dim=512, latent_dim=256,\n",
    "                 tb_imgs_to_display=4)\n",
    "        \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        validator = tf.cast(nov_cnn.variational_lower_bound, tf.int32) # alibi\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = optimizer.compute_gradients(-nov_cnn.variational_lower_bound)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        loss_summary = tf.summary.scalar(\"loss\", -nov_cnn.variational_lower_bound)\n",
    "        \n",
    "        all_sums_merged = tf.summary.merge_all()\n",
    "        \n",
    "        init = tf.global_variables_initializer()        \n",
    "        sess.run(init)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(trainlogpath)\n",
    "        valid_writer = tf.summary.FileWriter(testlogpath)\n",
    "        train_writer.add_graph(sess.graph)\n",
    "\n",
    "        mb_until = bs\n",
    "        ep = 0\n",
    "        step = 0\n",
    "        X_valid_10, _ = sample_some(valid_size)\n",
    "\n",
    "        while True:\n",
    "            if (step%resample_interval == 0):\n",
    "                X_all_10, _ = sample_some(train_size)\n",
    "\n",
    "            x_batch = X_all_10[mb_until-bs:mb_until]\n",
    "\n",
    "            _, step, summ, lo = sess.run([train_op,\n",
    "                                                   global_step,\n",
    "                                                   all_sums_merged,\n",
    "                                                   nov_cnn.variational_lower_bound],\n",
    "                                                  feed_dict={nov_cnn.X_10: x_batch, nov_cnn.keep_prob: kp})        \n",
    "\n",
    "            if (step%recording_interval == 0):\n",
    "                train_writer.add_summary(summ, step)  \n",
    " #               step, summ, lo = sess.run([global_step,\n",
    " #                                           all_sums_merged,\n",
    " #                                           nov_cnn.variational_lower_bound],\n",
    " #                                          feed_dict={nov_cnn.X_10: X_valid_10, nov_cnn.keep_prob: 1.0})\n",
    " #               valid_writer.add_summary(summ, step)    \n",
    "\n",
    "            mb_until += bs\n",
    "            if mb_until == train_size:\n",
    "                mb_until = bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
